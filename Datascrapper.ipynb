{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling and Scraping using Beautiful Soup\n",
    "Beautiful Soup library was used to crawl the website and scrape, clean and organize the data into a pandas dataframe. After which, the dataframe was written into a 'CSV' file.\n",
    "\n",
    "Beautiful Soup enabled us to create objects out of webpages. To extract attributes from each player, the player URL was used to crawl across webpages. Beautiful Soup provides us with functions like find, find_all to extract data from the html script according to the classes they are organizedin in the web script. The data was extracted and organized into a dictionary. Attributes such as height, weight, player skills, etc were extracted subsequently.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def soup_maker(url):\n",
    "    '''\n",
    "    Returns a beautiful soup object of the webpage when the webpage URL is passed. The data is scraped\n",
    "    from the beautiful soup object by the rest of the functions\n",
    "    '''\n",
    "    assert isinstance(url,str)\n",
    "    r = requests.get(url)\n",
    "    markup = r.content\n",
    "    soup = bs(markup, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def playerall(soup):\n",
    "    '''\n",
    "    Finds the URL of players from the soup object. Passes the URL to player_all_details which returns a dictionary\n",
    "    with all the player details\n",
    "    '''\n",
    "    assert isinstance(soup,bs)\n",
    "    \n",
    "    final_details = {}\n",
    "    table = soup.find('table', {'class': 'table table-hover persist-area'})\n",
    "    tbody = table.find('tbody')\n",
    "    all_a = tbody.find_all('a')\n",
    "    result=[]\n",
    "    for player in all_a:\n",
    "        final_details['short_name'] = player.text\n",
    "        if player['href'][0:8] == '/player/':\n",
    "            final_details.update(player_all_details('http://sofifa.com' + player['href']))\n",
    "            dict2 = final_details.copy()\n",
    "            result.append(dict2)\n",
    "    return result\n",
    "\n",
    "def player_all_details(url):\n",
    "    '''\n",
    "    Takes url of a player as the input. Returns all the player details using other like, playerskills.\n",
    "    '''\n",
    "    assert isinstance(url,str)\n",
    "    all_details = {}\n",
    "    soup = soup_maker(url)\n",
    "    player_info = soup.find('div', {'class': 'player'})\n",
    "    all_details.update(player_basic(player_info))   \n",
    "    player_stats = soup.find('div', {'class': 'stats'})\n",
    "    all_details.update(player_primary(player_stats))\n",
    "    secondary_info = soup.find('div', {'class': 'teams'})\n",
    "    all_details.update(player_secondary(secondary_info))\n",
    "    dict1=playerskills(url)\n",
    "    all_details.update(dict1)\n",
    "    return(all_details)\n",
    "\n",
    "\n",
    "def player_basic(soup):\n",
    "    '''\n",
    "    Returns the age, height, weight and preferred position of the player\n",
    "    '''\n",
    "    \n",
    "    player_data = {}\n",
    "    player_data['image'] = soup.find('img')['data-src']\n",
    "    player_data['full_name'] = soup.find('h1').text.split(' (')[0]\n",
    "    span = soup.find('span', attrs={'class': None}).text.strip()\n",
    "    dob = re.search('(\\(.*)\\)', span).group(0)\n",
    "    player_data['dob'] = dob.replace('(', '').replace(')', '')\n",
    "    infos = span.replace(dob + ' ', '').split(' ')\n",
    "    infos.append(infos[-1])\n",
    "    infos[-2] = infos[-3]\n",
    "    infos[-3] = infos[-4]\n",
    "    infos[-4] = infos[-5][-3:-1]\n",
    "    infos[-4] = infos[-4]+infos[-5][-1]\n",
    "    infos[-5] = infos[-5][:-3]\n",
    "    player_data['pref_pos'] = infos[:infos.index('Age')]\n",
    "    player_data['age'] = int(infos[infos.index('Age') + 1: -2][0])\n",
    "    player_data['height'] = int((infos[infos.index('Age') + 2: -1][0]).replace('cm', ''))\n",
    "    player_data['weight'] = int((infos[infos.index('Age') + 3:][0]).replace('kg', ''))\n",
    "    return(player_data)\n",
    "\n",
    "\n",
    "def player_primary(soup):\n",
    "    '''\n",
    "    Returns the palyers rating, potential, value and wage\n",
    "    '''\n",
    "    #assert isinstance(soup,bs)\n",
    "    player_data = {}\n",
    "    info = re.findall('\\d+', soup.text)\n",
    "    player_data['rating'] = int(info[0])\n",
    "    player_data['potential'] = int(info[1])\n",
    "    player_data['value'] = int(info[2])\n",
    "    if len(info)== 5:\n",
    "        player_data['wage'] = int(info[4])*1000\n",
    "    else:\n",
    "        player_data['wage'] = int(info[3])*1000\n",
    "    return(player_data)\n",
    "\n",
    "def player_secondary(soup):\n",
    "    '''\n",
    "    Returns the players country and preferred foot\n",
    "    '''\n",
    "    #assert isinstance(soup,bs)\n",
    "    player_data = {}\n",
    "    #print(soup)\n",
    "    player_data['preff_foot'] = soup.find('label', text='Preferred Foot')\\\n",
    "        .parent.contents[2].strip('\\n ')\n",
    "    temp = ''.join([i for i in str(soup)])\n",
    "    temp1 = temp.split('>')\n",
    "    temp2 = [i[i.find('title')+6:] for i in temp1 if i[:len('\\n<a href=\"/teams?')]=='\\n<a href=\"/teams?' ]       \n",
    "    player_data['country'] = temp2\n",
    "    return(player_data)\n",
    "\n",
    "\n",
    "def playerskills(url):\n",
    "    '''\n",
    "    Returns all the player skills like tackling, finishing, shot power, crossing, volley etc\n",
    "    '''\n",
    "    assert isinstance(url,str)\n",
    "    all_details = {}\n",
    "    soup = soup_maker(url)\n",
    "    a = soup.find_all('div', {'class': 'mb-20'})\n",
    "    li = []\n",
    "    k=0\n",
    "    for i in a:\n",
    "        b = i.find_all('li')\n",
    "        for j in b:\n",
    "            li.append(j.text)\n",
    "    dict1={}\n",
    "    j=0\n",
    "    for i in li:\n",
    "        if j>=34:\n",
    "            break\n",
    "        a=i.split()\n",
    "        if len(a)==2:\n",
    "            dict1.update({str(a[1]):int(a[0])})\n",
    "        else:\n",
    "            key=str(a[-2])+str(a[-1])\n",
    "            dict1.update({key:int(a[0])})\n",
    "        j+=1\n",
    "        \n",
    "    return dict1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finds all the player details from the main webpage of sofifa\n",
    "### Stores all the player information in a dictionary\n",
    "\n",
    "result_final=[]\n",
    "players={}\n",
    "for i in range(0):\n",
    "    print(i)\n",
    "    url = 'http://sofifa.com/players?offset='+str(80*i)\n",
    "    soup = soup_maker(url)\n",
    "    result=playerall(soup)\n",
    "    for i in range (0, len(result)):\n",
    "        players.update({result[i]['full_name']:result[i]})\n",
    "    print(\"Length of players is\", len(players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converts the dataset into a panda dataframe and writes into a CSV file\n",
    "import pandas as pd\n",
    "playerlist=[(v) for k, v in players.items()]\n",
    "my_df = pd.DataFrame(playerlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv('players1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
